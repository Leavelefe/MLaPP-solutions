{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Principal Component Analysis versus Factor Analysis\n",
    "\n",
    "Consider $\\mathbf{z}_i \\sim \\mathcal{N}(0,I)$. Let \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_i = W\\mathbf{z}_i,~\\text{where}~\n",
    "W = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 0.001 \\\\\n",
    "0 & 10\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Let us generate 1,000 observations of such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.29485409,   0.29536213,   5.08037685],\n",
       "       [  0.7047403 ,   0.70556643,   8.26130625],\n",
       "       [  2.13545214,   2.1348843 ,  -5.67838211],\n",
       "       [  0.73523438,   0.73426439,  -9.69995451],\n",
       "       [  0.69423613,   0.69560746,  13.71332032],\n",
       "       [ -1.50154155,  -1.50107505,   4.66498411],\n",
       "       [  1.48276751,   1.48449822,  17.30708419],\n",
       "       [ -0.80440668,  -0.80654125, -21.3457542 ],\n",
       "       [ -0.49926355,  -0.49931611,  -0.52567443],\n",
       "       [ -0.40167833,  -0.40209348,  -4.15155475]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(2016)\n",
    "\n",
    "N = 1000\n",
    "Z = stats.norm.rvs(loc=0, scale=1, size=2*N)\n",
    "Z = Z.reshape((-1,2))\n",
    "W = np.array([[1,0],[1,0.001],[0,10]])\n",
    "X = np.dot(Z, W.T)\n",
    "X[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply probabilistic principal component analysis, now, with $L = 1$. Here, it's assumed that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_i \\sim \\mathcal{N}\\left(Wz_i,\\sigma^2I\\right).\n",
    "\\end{equation}\n",
    "\n",
    "We can estimate the parameters with Theorem 12.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.01111435]\n",
      " [ -0.01212571]\n",
      " [-10.11361729]]\n",
      "0.99382893984\n"
     ]
    }
   ],
   "source": [
    "L = 1\n",
    "U,D,V = np.linalg.svd(np.dot(X.T,X)/N)\n",
    "V = V.T\n",
    "sigma2_hat = sum(D[L:])/(len(D) - L)\n",
    "W = np.dot(V[:,:L],np.diag(np.sqrt(D[:L] - sigma2_hat)))\n",
    "print(W)\n",
    "print(sigma2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that this points along $x_3$, which is the direction of highest variance. The sign doesn't really mean much since the normal distribution is symmetric, so we have an identifiability problem.\n",
    "\n",
    "Now, let's try factor analysis. This time, we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_i \\sim \\mathcal{N}\\left(Wz_i + \\boldsymbol\\mu,\\Psi\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Psi$ is diagonal. We can estimate $W$ and $\\Psi$ with the EM algorithm described in 12.1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84388586]\n",
      " [ 0.84389397]\n",
      " [ 0.08117066]]\n",
      "[-0.03664242 -0.03669304 -0.50622523]\n",
      "[[  5.15256797e-07   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   5.14875191e-07   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.03013188e+02]]\n"
     ]
    }
   ],
   "source": [
    "## initialize\n",
    "W = stats.uniform.rvs(size=L*X.shape[1])\n",
    "W = W.reshape((-1,L))\n",
    "W = np.hstack((W, np.zeros(shape=(X.shape[1],1))))\n",
    "Psi = np.diag(np.diag(np.dot(X.T, X)))/len(X)\n",
    "\n",
    "\n",
    "def update_W(X, B, C):\n",
    "    numerator = np.zeros(shape=(X.shape[1], B.shape[1]))\n",
    "    denominator = np.zeros_like(C[0])\n",
    "    for i in range(len(X)):\n",
    "        numerator += np.outer(X[i], B[i])\n",
    "        denominator += C[i]\n",
    "    denominator\n",
    "    return np.dot(numerator, np.linalg.inv(denominator))\n",
    "\n",
    "def update_Psi(X, W, B):\n",
    "    Psi = np.zeros(shape=(X.shape[1], X.shape[1]))\n",
    "    for i in range(len(X)):\n",
    "        Psi += np.outer(X[i] - np.dot(W, B[i]), X[i])\n",
    "    return np.diag(np.diag(Psi))/len(X)\n",
    "\n",
    "for k in range(100): # 50 iterations should be enough\n",
    "    # E step\n",
    "    Sigma = np.linalg.inv(np.eye(L) + np.dot(np.dot(W[:,:L].T, np.linalg.inv(Psi)), W[:,:L]))\n",
    "    m = np.empty(shape=(N, L))\n",
    "    C = np.empty(shape=(N, L + 1, L + 1))\n",
    "    for i in range(N):\n",
    "        m[i] = np.dot(Sigma, np.dot(np.dot(W[:,:L].T, np.linalg.inv(Psi)), X[i] - W[:,L]))        \n",
    "        C[i][:L,:L] = Sigma + np.outer(m[i], m[i])\n",
    "        C[i][L,:L] = m[i]\n",
    "        C[i][:L,L] = m[i]\n",
    "        C[i][L,L] = 1\n",
    "    B = np.hstack((m, np.ones((N,1))))  \n",
    "    # M step\n",
    "    W = update_W(X, B, C)\n",
    "    Psi = update_Psi(X, W, B)\n",
    "print(W[:,:L]) # principal directions\n",
    "print(W[:,L]) # mu\n",
    "print(Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that the direction of this vector is approximately $(1,1,0)$, which is the direction of max correlation. Recall that \n",
    "\n",
    "\\begin{align}\n",
    "x_{i1} &= z_{i1} \\\\\n",
    "x_{i2} &= z_{i1} + 0.001z_{i2} \\\\\n",
    "x_{i3} &= 10z_{i2},\n",
    "\\end{align}\n",
    "\n",
    "so $x_{i1}$ and $x_{i2}$ are nearly identical, so they are very correlated. $x_{i3}$ can almost be treated as an indepedent normal variable. \n",
    "\n",
    "The main difference between the two methods is $\\Psi$, for factor analysis, we have that $\\Psi$ is merely diagonal, whereas, PPCA requires that $\\Psi = \\sigma^2I$.\n",
    "\n",
    "In this case, nearly all the variance is found in $x_{i3}$ since the coefficient is much larger. Thus, that direction explains the variance, so it's no surprise that our weight vector found in PPCA points in that direction. In this way, it's almost like when we project onto this direction, we're just observing a single-dimensional normal distribution. From another perspective, if we were going to try to reconstruct $\\mathbf{x}_i$ with one dimension, we'd simply take the sample standard deviation of the third dimension since that has the largest variance and multiply it by a standard normal.\n",
    "\n",
    "On the other hand, that single-dimensional normal distribution can be seen as independent from $x_{i1}$ and $x_{i2}$. Thus, we can explain everything that happens along that dimension with $\\Psi$ by setting the variance for the third component high. In this way, we have maximized the likelihood for that dimension, so all that is left is to explain relationship between $x_{i1}$ and $x_{i2}$. They happen to be nearly identical, so the weight vector approximately points in the direction $(1,1,0)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
